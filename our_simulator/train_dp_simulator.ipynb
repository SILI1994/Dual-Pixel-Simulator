{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c49812-8a5d-4c08-9c5b-b6752f7df930",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model_our_sim import DPSimulator\n",
    "import numpy as np\n",
    "from loss import DPLoss, EdgeLoss\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from dataset import MyDataset, collate_fn_replace_corrupted\n",
    "import functools\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9076239-59b1-402f-b78c-f7a6f4d1b0be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_h5py_file(name, my_dict):\n",
    "    h = h5py.File(name, 'w')\n",
    "    for k, v in my_dict.items():\n",
    "        h.create_dataset(k, data=np.array([v]).squeeze())\n",
    "    h.close()\n",
    "\n",
    "\n",
    "\n",
    "def MAE_PSNR_SSIM(batch_img_1, batch_img_2, reduction='sum', mask=None):\n",
    "    def _my_mae(_x, _y, _mask):\n",
    "        _diff = np.abs(_x - _y) * _mask\n",
    "        return _diff[np.nonzero(_mask)].mean()\n",
    "\n",
    "    def _my_ssim(_x, _y, _win_size=7, _data_range=1.0, _mask=None):\n",
    "        _, _ssim_mat = structural_similarity(_x, _y, data_range=_data_range, multichannel=True, win_size=_win_size, full=True)\n",
    "        _pad = (_win_size - 1) // 2\n",
    "        _ssim_mat = _ssim_mat[_pad:-_pad, _pad:-_pad] * _mask[_pad:-_pad, _pad:-_pad]\n",
    "        return _ssim_mat[np.nonzero(_mask[_pad:-_pad, _pad:-_pad])].mean()\n",
    "\n",
    "    def _my_psnr(_x, _y, _data_range=1.0, _mask=None):\n",
    "        _diff = ((_x - _y) ** 2) * _mask\n",
    "        return 10 * np.log10((_data_range ** 2) / _diff[np.nonzero(_mask)].mean())\n",
    "\n",
    "    batch_img_1, batch_img_2 = torch.clip(batch_img_1, 0, 1), torch.clip(batch_img_2, 0, 1)\n",
    "    batch_1, batch_2 = batch_img_1.detach().cpu().numpy(), batch_img_2.detach().cpu().numpy(),\n",
    "    batch_1, batch_2 = batch_1.transpose(0, 2, 3, 1), batch_2.transpose(0, 2, 3, 1)\n",
    "    if mask is None:\n",
    "        batch_mask = np.ones_like(batch_1)\n",
    "    else:\n",
    "        batch_mask = mask.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    mae, psnr, ssim = [], [], []\n",
    "    for x, y, m in zip(batch_1, batch_2, batch_mask):\n",
    "        mae.append(_my_mae(x, y, _mask=m))\n",
    "        psnr.append(_my_psnr(x, y, _mask=m))\n",
    "        ssim.append(_my_ssim(x, y, _mask=m))\n",
    "    if reduction == 'sum':\n",
    "        return np.sum(mae), np.sum(psnr), np.sum(ssim)\n",
    "    elif reduction == 'mean':\n",
    "        return np.mean(mae), np.mean(psnr), np.mean(ssim)\n",
    "    \n",
    "    \n",
    "\n",
    "def norm_dep(dep):\n",
    "    all_new_dep = torch.zeros_like(dep)\n",
    "    for i, x in enumerate(dep):\n",
    "        curr_mask = x != 0\n",
    "        x[x == 0] = x.max()\n",
    "        new_dep = (x - x.min()) / (x.max() - x.min())\n",
    "        new_dep[curr_mask == 0] = -1\n",
    "        all_new_dep[i] = new_dep\n",
    "    return all_new_dep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314a0701-1c2c-4063-b547-6f0cab784294",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(args):\n",
    "    ## dataloader\n",
    "    train_set = MyDataset(args.data_dir, partition='train', required_dep_percent=args.required_dep_percent)\n",
    "    val_set = MyDataset(args.data_dir, partition='valid')\n",
    "    train_loader = DataLoader(train_set, batch_size=args.bs, shuffle=True, num_workers=args.n_worker, drop_last=True,\n",
    "                              collate_fn=functools.partial(collate_fn_replace_corrupted, torch_dataset=train_set))\n",
    "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=args.n_worker, drop_last=False)\n",
    "    print('train size: {}, validation size: {}'.format(train_set.__len__(), val_set.__len__()))\n",
    "\n",
    "    ## initialization\n",
    "    model = DPSimulator(k_size=5)\n",
    "    model = model.to(args.device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.n_epoch, eta_min=1e-6)\n",
    "    criterion_dp = DPLoss(loss_type=args.loss_type, use_mask=True)\n",
    "    criterion_edge = EdgeLoss(device=args.device, use_mask=True)\n",
    "    min_train_loss, min_val_loss, min_val_mae, max_val_ssim, max_val_psnr = float('inf'), float('inf'), float('inf'), 0, 0\n",
    "    logger = open('log_{}.txt'.format(args.task), 'w').close()\n",
    "    Path('./checkpoints').mkdir(exist_ok=True, parents=True)\n",
    "    print('init done')\n",
    "\n",
    "    ## loop\n",
    "    for epoch in range(args.n_epoch):\n",
    "        ## train phase\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        curr_train_loss, curr_train_mae, curr_train_psnr, curr_train_ssim = 0, 0, 0, 0\n",
    "        num_train = 0\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader):\n",
    "            sharp, dep, coc = data['sharp'].to(args.device), data['dep'].to(args.device), data['coc'].to(args.device)\n",
    "            dp_l, dp_r = data['dp_l'].to(args.device), data['dp_r'].to(args.device)\n",
    "            mask = torch.where(dep == 0, 0, 1)\n",
    "            dep = norm_dep(dep)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_l, pred_r, _, _ = model(sharp, dep, coc)\n",
    "                loss = criterion_dp(pred_l, pred_r, dp_l, dp_r, mask) + criterion_edge(pred_l, dp_l, mask) + criterion_edge(pred_r, dp_r, mask)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            curr_train_loss += loss.item()\n",
    "            mae_l, psnr_l, ssim_l = MAE_PSNR_SSIM(pred_l, dp_l, 'sum', mask)\n",
    "            mae_r, psnr_r, ssim_r = MAE_PSNR_SSIM(pred_r, dp_r, 'sum', mask)\n",
    "            mae, psnr, ssim = (mae_l + mae_r) / 2, (psnr_l + psnr_r) / 2, (ssim_l + ssim_r) / 2\n",
    "            curr_train_mae, curr_train_psnr, curr_train_ssim = curr_train_mae + mae, curr_train_psnr + psnr, curr_train_ssim + ssim\n",
    "            num_train += sharp.shape[0]\n",
    "            \n",
    "            scaler.update()\n",
    "                \n",
    "        scheduler.step()\n",
    "        min_train_loss = min(min_train_loss, curr_train_loss)\n",
    "        curr_train_mae, curr_train_psnr, curr_train_ssim = curr_train_mae / num_train, curr_train_psnr / num_train, curr_train_ssim / num_train\n",
    "        print('Epoch: {}, curr_train_loss: {:.5f}, curr_train_mae: {:.5f}, curr_train_psnr: {:.5f}, '\n",
    "              'curr_train_ssim: {:.5f}'.format(epoch, curr_train_loss, curr_train_mae, curr_train_psnr, curr_train_ssim))\n",
    "\n",
    "\n",
    "        ## validation phase\n",
    "        curr_val_loss, curr_val_mae, curr_val_psnr, curr_val_ssim = 0, 0, 0, 0\n",
    "        num_val = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(val_loader):\n",
    "                sharp, dep, coc = data['sharp'].to(args.device), data['dep'].to(args.device), data['coc'].to(args.device)\n",
    "                dp_l, dp_r = data['dp_l'].to(args.device), data['dp_r'].to(args.device)\n",
    "                mask = torch.where(dep == 0, 0, 1)\n",
    "                dep = norm_dep(dep)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred_l, pred_r, _, _ = model(sharp, dep, coc)\n",
    "                    loss = criterion_dp(pred_l, pred_r, dp_l, dp_r, mask) + criterion_edge(pred_l, dp_l, mask) + criterion_edge(pred_r, dp_r, mask)\n",
    "                    \n",
    "                curr_val_loss += loss.item()\n",
    "                mae_l, psnr_l, ssim_l = MAE_PSNR_SSIM(pred_l, dp_l, 'sum', mask)\n",
    "                mae_r, psnr_r, ssim_r = MAE_PSNR_SSIM(pred_r, dp_r, 'sum', mask)\n",
    "                mae, psnr, ssim = (mae_l + mae_r) / 2, (psnr_l + psnr_r) / 2, (ssim_l + ssim_r) / 2\n",
    "                num_val += sharp.shape[0]\n",
    "                curr_val_mae, curr_val_psnr, curr_val_ssim = curr_val_mae + mae, curr_val_psnr + psnr, curr_val_ssim + ssim\n",
    "            \n",
    "            ## checkpoints\n",
    "            min_val_loss = min(min_val_loss, curr_val_loss)\n",
    "            curr_val_mae, curr_val_psnr, curr_val_ssim = curr_val_mae / num_val, curr_val_psnr / num_val, curr_val_ssim / num_val\n",
    "            if max_val_ssim < curr_val_ssim:\n",
    "                max_val_ssim = curr_val_ssim\n",
    "                torch.save(model.state_dict(), './checkpoints/{}_max_val_ssim.cp'.format(args.task))\n",
    "            if max_val_psnr < curr_val_psnr:\n",
    "                max_val_psnr = curr_val_psnr\n",
    "                torch.save(model.state_dict(), './checkpoints/{}_max_val_psnr.cp'.format(args.task))\n",
    "            if min_val_mae > curr_val_mae:\n",
    "                min_val_mae = curr_val_mae\n",
    "                torch.save(model.state_dict(), './checkpoints/{}_min_val_mae.cp'.format(args.task))\n",
    "          \n",
    "        \n",
    "        ## logger to txt\n",
    "        print('Epoch: {}, curr_val_loss: {:.5f}, min_val_loss: {:.5f}, curr_val_mae: {:.5f}, min_val_mae: {:.5f}, curr_val_psnr: {:.5f}, max_val_psnr: {:.5f}, '\n",
    "              'curr_val_ssim: {:.5f}, max_val_ssim: {:.5f}'.format(epoch, curr_val_loss, min_val_loss, curr_val_mae, min_val_mae, curr_val_psnr, max_val_psnr, curr_val_ssim, max_val_ssim))\n",
    "        \n",
    "        f = open('log_{}.txt'.format(args.task), 'a')\n",
    "        f.write('Epoch: {}, curr_train_loss: {:.5f}, curr_train_mae: {:.5f}, curr_train_psnr: {:.5f}, curr_train_ssim: {:.5f}, curr_val_loss: {:.5f}, '\n",
    "                'min_val_loss: {:.5f}, curr_val_mae: {:.5f}, min_val_mae: {:.5f}, curr_val_psnr: {:.5f}, max_val_psnr: {:.5f}, curr_val_ssim: {:.5f}, '\n",
    "                'max_val_ssim: {:.5f}\\n'.format(epoch, curr_train_loss, curr_train_mae, curr_train_psnr, curr_train_ssim, curr_val_loss, min_val_loss, \n",
    "                 curr_val_mae, min_val_mae, curr_val_psnr, max_val_psnr, curr_val_ssim, max_val_ssim))\n",
    "        f.close()\n",
    "\n",
    "    print('training finished')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787811da-16d7-422d-85e8-4e14a140d5cd",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # train-related\n",
    "    parser.add_argument('--task', type=str, default='DP_simulator', help='task name')\n",
    "    parser.add_argument('--device', type=str, default='cuda:5', help='cuda device')\n",
    "    parser.add_argument('--n_epoch', type=int, default=100, help='number of epochs')\n",
    "    parser.add_argument('--bs', type=int, default=8, help='batch size')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "    parser.add_argument('--loss_type', type=str, default='charbonnier', help='loss type')\n",
    "    # others\n",
    "    parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "    parser.add_argument('--data_dir', type=str, default='/dataset/workspace2021/li/final_data', help='data directory')\n",
    "    parser.add_argument('--n_worker', type=int, default=8, help='number of workers')\n",
    "    parser.add_argument('--required_dep_percent', type=float, default=0.8, help='required percent of valid depths of a patch in training')\n",
    "    _args = parser.parse_args(args=[])\n",
    "\n",
    "    # fix seed\n",
    "    np.random.seed(_args.seed)\n",
    "    torch.manual_seed(_args.seed)\n",
    "    random.seed(_args.seed)\n",
    "    if _args.device != 'cpu':\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # train\n",
    "    train(_args)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
